# -*- coding: utf-8 -*-
"""Bert-AG-News-Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cyTs60wGoI8mvnVQaLRSgoar5imkq5pI

## AG-News Text Classification using BERT:

In this project, we will cover in detail the application of BERT base model with respect to text classification.
We will witness how this state of the art Transformer model is able to achieve extremely high performance metrics with respect to a large corpus of data comprising of more than 100k+ labelled training examples.
The hugging face transformer & dataset library along with ktrain (a high level python wrapper with tensorflow backend) will be used to build, train & fine tune the BERT model with respect to classification on this custom dataset.

### Checking Hardware Acceleration:
"""

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Select the Runtime > "Change runtime type" menu to enable a GPU accelerator, ')
  print('and then re-execute this cell.')
else:
  print(gpu_info)

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

"""### Install Libraries:"""

!pip install ktrain
!pip install transformers
!pip install datasets

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import ktrain
from ktrain import text
import tensorflow as tf
from sklearn.model_selection import train_test_split
from datasets import list_datasets
from datasets import load_dataset
import timeit

print("Tensorflow version : ", tf.__version__)
print("GPU available : ",bool(tf.test.is_gpu_available))
print("GPU name : ",tf.test.gpu_device_name())

"""### Checking available datasets in Hugging Face:"""

available_datasets = list_datasets()
print("Count of available datasets : ", len(available_datasets))
print()
print("<====== Dataset List ======> :\n")
print('\n  |__ '.join(dataset for dataset in available_datasets))

"""### Import AG News Dataset:"""

ag_news_dataset = load_dataset('ag_news')
print("\n", ag_news_dataset)

"""### Dataset Details:

AG is a collection of more than 1 million news articles. News articles have been gathered from more than 2000 news sources by ComeToMyHead in more than 1 year of activity. ComeToMyHead is an academic news search engine which has been running since July, 2004. The dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc), information retrieval (ranking, search, etc), xml, data compression, data streaming, and any other non-commercial activity. For more information, please refer to the link http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html

AG News (AG’s News Corpus) is a subdataset of AG's corpus of news articles constructed by assembling titles and description fields of articles from the 4 largest classes (“World”, “Sports”, “Business”, “Sci/Tech”) of AG’s Corpus. The AG News contains 30,000 training and 1,900 test samples per class.
"""

print("Dataset Items: \n", ag_news_dataset.items())
print("\nDataset type: \n", type(ag_news_dataset))
print("\nShape of dataset: \n", ag_news_dataset.shape)
print("\nNo of rows: \n", ag_news_dataset.num_rows)
print("\nNo of columns: \n", ag_news_dataset.num_columns)

print("\nColumn Names: \n", ag_news_dataset.column_names)
print("\n", ag_news_dataset.data)

print(ag_news_dataset['train'][0])
print(ag_news_dataset['train'][1])

print(ag_news_dataset['train']['text'][0])
print(ag_news_dataset['train']['label'][0])
print()
print(ag_news_dataset['train']['text'][35000])
print(ag_news_dataset['train']['label'][35000])
print()
print(ag_news_dataset['train']['text'][60000])
print(ag_news_dataset['train']['label'][60000])
print()
print(ag_news_dataset['train']['text'][100000])
print(ag_news_dataset['train']['label'][100000])

"""### Loading Train & Test Datasets:"""

ag_news_train = load_dataset('ag_news', split='train')
ag_news_test = load_dataset('ag_news', split='test')
print("Train Dataset : ", ag_news_train.shape)
print("Test Dataset : ", ag_news_test.shape)

print(ag_news_train[0])
print(ag_news_test[0])

print("\nTrain Dataset Features: \n", ag_news_train.features)
print("\nTest Dataset Features: \n", ag_news_test.features)

"""### Creating DataFrame object for K-train:"""

pd.set_option('Display.max_columns', None)
ag_news_train_df = pd.DataFrame(data=ag_news_train)
ag_news_train_df.head(10)

ag_news_train_df.tail(10)

ag_news_test_df = pd.DataFrame(data=ag_news_test)
ag_news_test_df.head(10)

ag_news_test_df.tail(10)

"""### Data Preprocessing:"""

class_label_names = ['World', 'Sports', 'Business', 'Sci/Tech']

(X_train, y_train), (X_test, y_test), preprocessing_var = text.texts_from_df(train_df=ag_news_train_df,
                                                                             text_column='text',
                                                                             label_columns='label',
                                                                             val_df=ag_news_test_df,
                                                                             maxlen=512,
                                                                             preprocess_mode='bert')

"""### Create BERT Model:"""

transformer_bert_model = text.text_classifier(name='bert',
                                              train_data=(X_train, y_train),
                                              preproc=preprocessing_var)

transformer_bert_model.layers

"""### Compile and train Bert in a Learner Object:"""

bert_learner = ktrain.get_learner(model=transformer_bert_model,
                            train_data=(X_train, y_train),
                            val_data=(X_test, y_test),
                            batch_size=6)

"""### Best Hyper-parameters for BERT:
• Batch size: 16, 32

• Learning rate: 5e-5, 3e-5, 2e-5

• Number of epochs: 2, 3, 4

### Train BERT on AG-News dataset:
"""

training_start_time = timeit.default_timer()
bert_learner.fit_onecycle(lr=2e-5, epochs=3)
training_stop_time = timeit.default_timer()

print("Total training time in minutes: \n", (training_stop_time - training_start_time)/60)  
print("Total training time in hours: \n", (training_stop_time - training_start_time)/3600)

"""### Checking BERT performance metrics:"""

bert_learner.validate()

bert_learner.validate(class_names=class_label_names)

"""### Saving the model:"""

bert_predictor = ktrain.get_predictor(bert_learner.model, preproc=preprocessing_var)
bert_predictor.get_classes()

bert_predictor.save('/content/bert-ag-news-predictor')

!zip -r /content/bert-ag-news-predictor.zip /content/bert-ag-news-predictor

"""### Re-loading Model:"""

bert_predictor_2 = ktrain.load_predictor('/content/bert-ag-news-predictor')
bert_predictor_2.get_classes()

"""### References:
* https://huggingface.co/
* https://arxiv.org/abs/1810.04805
"""

